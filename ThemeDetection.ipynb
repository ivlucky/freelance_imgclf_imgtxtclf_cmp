{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "ThemeDetection.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BfFKGKh2tHQL"
      },
      "source": [
        "# Install packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G4N2SVvKtGzW"
      },
      "source": [
        "!sudo apt install tesseract-ocr\n",
        "!pip install pytesseract==0.3.8\n",
        "!pip install transformers==4.11.3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oAdpXQBnaX1e"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DC6PAaz3aVQt"
      },
      "source": [
        "from __future__ import print_function, division\n",
        "import torch\n",
        "import torchvision\n",
        "from tqdm import tqdm, tqdm_notebook\n",
        "from os.path import exists\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision import datasets, models\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import time\n",
        "import os\n",
        "import copy\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "import torch.onnx\n",
        "import cv2\n",
        "import seaborn as sns\n",
        "import pytesseract\n",
        "import pandas as pd\n",
        "pytesseract.pytesseract.tesseract_cmd = r'/usr/bin/tesseract'\n",
        "%matplotlib inline\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "plt.ion() "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XmNk4x9fadoT"
      },
      "source": [
        "# CUDA status"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QlIrcmpR25jY"
      },
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "device, torch.cuda.is_available()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_faQGs7u5nES"
      },
      "source": [
        "# Более новая версия торча не совместима с ГПУ на видеокартах Colab\n",
        "# https://pytorch.org/ 10.1\n",
        "cuda_output = !ldconfig -p|grep cudart.so|sed -e 's/.*\\.\\([0-9]*\\)\\.\\([0-9]*\\)$/cu\\1\\2/'\n",
        "accelerator = cuda_output[0] if exists('/dev/nvidia0') else 'cpu'\n",
        "print(\"\\ndevice: \", device, \"\\nPyTorch Version: \", torch.__version__, \"\\nTorchvision Version: \", torchvision.__version__, \\\n",
        "    \"\\nПроверяем, доступны ли GPU: \", torch.cuda.is_available(), \"\\naccelerator: \", accelerator)\n",
        "if torch.cuda.is_available() == False:\n",
        "    !pip install torch==1.7.1+cu101 torchvision==0.8.2+cu101 torchaudio==0.7.2 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "\n",
        "# Проверяем, доступны ли GPU\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "print(\"\\ndevice: \", device, \"\\nPyTorch Version: \", torch.__version__, \"\\nTorchvision Version: \", torchvision.__version__, \\\n",
        "    \"\\nПроверяем, доступны ли GPU: \", torch.cuda.is_available(), \"\\naccelerator: \", accelerator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y_GGb3h55p3h"
      },
      "source": [
        "print(torch.__config__.show()) \n",
        "print(torch.version.cuda)\n",
        "torch.cuda.is_available()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gRtpB1qHjHla"
      },
      "source": [
        "# 0) Work with data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RwHfdtQGsAHI"
      },
      "source": [
        "data_dir=\"/content/gdrive/MyDrive/data/dataset\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "prGwwu4DsDip"
      },
      "source": [
        "data_transforms = {\n",
        "    'train': transforms.Compose([\n",
        "        transforms.RandomResizedCrop(540),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
        "    ]),\n",
        "    'test': transforms.Compose([\n",
        "        transforms.RandomResizedCrop(540),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
        "    ]),\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YpCnuXovsHDY"
      },
      "source": [
        "def get_dataset(data_dir, data_transforms, folders=['train', 'test']):\n",
        "    image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n",
        "                                              data_transforms[x])\n",
        "                      for x in folders}\n",
        "    dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=4,\n",
        "                                                 shuffle=True, num_workers=4)\n",
        "                  for x in folders}\n",
        "    dataset_sizes = {x: len(image_datasets[x]) for x in folders}\n",
        "    classes = image_datasets['train'].classes\n",
        "\n",
        "    return dataloaders[\"train\"], dataloaders['test'], classes, dataset_sizes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hbcN_j3ysJQS"
      },
      "source": [
        "trainloader, testloader, classes, dataset_sizes=get_dataset(data_dir,data_transforms, folders=['train', 'test'])\n",
        "print('Classes: ',  classes)\n",
        "print('The datasest have: ',  dataset_sizes ,\" images\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YnxfIPbJhtnR"
      },
      "source": [
        "# TODO: CHECK THAT SPLIT ARE VALID"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1BMooXkKsLaW"
      },
      "source": [
        "def imshow(img):\n",
        "    img = img / 2+0.5      \n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "dataiter = iter(trainloader)\n",
        "images, labels = next(dataiter)\n",
        "imshow(torchvision.utils.make_grid(images))\n",
        "print('|'.join('%10s' % classes[labels[j]] for j in range(4)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mA-9s-cwjRpo"
      },
      "source": [
        "# 1) Image classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PYb3xDM01-W3"
      },
      "source": [
        "def fit_epoch(model, train_loader, criterion, optimizer):\n",
        "    running_loss = 0.0\n",
        "    running_corrects = 0\n",
        "    processed_data = 0\n",
        "  \n",
        "    for inputs, labels in train_loader:\n",
        "        inputs = inputs.to(DEVICE)\n",
        "        labels = labels.to(DEVICE)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        preds = torch.argmax(outputs, 1)\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        running_corrects += torch.sum(preds == labels.data)\n",
        "        processed_data += inputs.size(0)\n",
        "              \n",
        "    train_loss = running_loss / processed_data\n",
        "    train_acc = running_corrects.cpu().numpy() / processed_data\n",
        "    return train_loss, train_acc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Af7Afk-o2AUV"
      },
      "source": [
        "def eval_epoch(model, val_loader, criterion):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    running_corrects = 0\n",
        "    processed_size = 0\n",
        "\n",
        "    for inputs, labels in val_loader:\n",
        "        i = 0\n",
        "        inputs = inputs.to(DEVICE)\n",
        "        labels = labels.to(DEVICE)\n",
        "\n",
        "        with torch.set_grad_enabled(False):\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            preds = torch.argmax(outputs, 1)\n",
        "\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        running_corrects += torch.sum(preds == labels.data)\n",
        "        if (preds[i] != labels.data[i]):\n",
        "          print(preds[i], labels.data[i])\n",
        "        i+=1\n",
        "        processed_size += inputs.size(0)\n",
        "    val_loss = running_loss / processed_size\n",
        "    val_acc = running_corrects.double() / processed_size\n",
        "    return val_loss, val_acc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XOEP4Fhi2De1"
      },
      "source": [
        "def train(train_loader, val_loader, model, criterion, epochs, batch_size,optimizer, scheduler, sampler = None, shuffle = True):\n",
        "\n",
        "    history = []\n",
        "    log_template = \"\\nEpoch {ep:03d} train_loss: {t_loss:0.4f} \\\n",
        "    val_loss {v_loss:0.4f} train_acc {t_acc:0.4f} val_acc {v_acc:0.4f}\"\n",
        "\n",
        "    with tqdm(desc=\"epoch\", total=epochs) as pbar_outer:\n",
        "        for epoch in range(epochs):\n",
        "            train_loss, train_acc = fit_epoch(model, train_loader, criterion, optimizer)\n",
        "            print(\"loss\", train_loss)\n",
        "            \n",
        "            val_loss, val_acc = eval_epoch(model, val_loader, criterion)\n",
        "            history.append((train_loss, train_acc, val_loss, val_acc))\n",
        "            scheduler.step()\n",
        "            pbar_outer.update(1)\n",
        "            tqdm.write(log_template.format(ep=epoch+1, t_loss=train_loss,\\\n",
        "                                           v_loss=val_loss, t_acc=train_acc, v_acc=val_acc))\n",
        "            \n",
        "    return history"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xiYv0UH42FSh"
      },
      "source": [
        "def predict(model, test_loader):\n",
        "    with torch.no_grad():\n",
        "        logits = []\n",
        "    \n",
        "        for inputs in test_loader:\n",
        "            inputs = inputs.to(DEVICE)\n",
        "            model.eval()\n",
        "            outputs = model(inputs).cpu()\n",
        "            logits.append(outputs)\n",
        "            \n",
        "    probs = nn.functional.softmax(torch.cat(logits), dim=-1).numpy()\n",
        "    return probs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8yqChvUE0Vux"
      },
      "source": [
        "myModel = models.googlenet(pretrained=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IUjL9ErLL-la"
      },
      "source": [
        "%%timeit\n",
        "myModel(images[:1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6nP-nkAd2JIG"
      },
      "source": [
        "n_classes = 9\n",
        "for param in myModel.parameters():\n",
        "  param.requires_grad = False\n",
        "DEVICE = torch.device(\"cuda\")\n",
        "numFeat = myModel.fc.in_features\n",
        "myModel.fc = nn.Linear(numFeat, n_classes)\n",
        "myModel = myModel.to(DEVICE)\n",
        "criterizator = nn.CrossEntropyLoss()\n",
        "optimizator = torch.optim.AdamW(myModel.parameters())\n",
        "shedulator = torch.optim.lr_scheduler.StepLR(optimizator,3,0.5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XkZBNzLU2hD8"
      },
      "source": [
        "%%time\n",
        "\n",
        "history = train(trainloader, testloader, model=myModel, criterion = criterizator, epochs=10, batch_size=40,optimizer = optimizator,scheduler = shedulator)\n",
        "\n",
        "for param in myModel.parameters():\n",
        "  param.requires_grad = True\n",
        "history = train(trainloader, testloader, model=myModel, criterion = criterizator, epochs=24, batch_size=40,optimizer = optimizator,scheduler = shedulator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4JbN4ygZi_PM"
      },
      "source": [
        "# 2) Image to string"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aHSLaMMlyE9m"
      },
      "source": [
        "img_files = []\n",
        "for path, subdirs, files in os.walk(data_dir):\n",
        "    for name in files:\n",
        "      img_files.append(os.path.join(path, name))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tV0LLRrh3UEM"
      },
      "source": [
        "def image_to_string(img_filepath):\n",
        "\n",
        "  img_cv = cv2.imread(img_filepath)\n",
        "  img_rgb = cv2.cvtColor(img_cv, cv2.COLOR_BGR2RGB)\n",
        "  return pytesseract.image_to_string(img_rgb)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "61fYZJ1-t4PF"
      },
      "source": [
        "%%time\n",
        "\n",
        "df_dict = {\n",
        "    'filepath': [],\n",
        "    'text': [],\n",
        "}\n",
        "\n",
        "for img in tqdm(img_files):\n",
        "  text = image_to_string(img)\n",
        "  df_dict['filepath'].append(img)\n",
        "  df_dict['text'].append(text)\n",
        "\n",
        "df_raw = pd.DataFrame(df_dict)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tvxOmLbB77O2"
      },
      "source": [
        "df_raw['type'] = df_raw['filepath'].str.split(data_dir).str[1].str.split('/').str[1]\n",
        "df_raw['label'] = df_raw['filepath'].str.split(data_dir).str[1].str.split('/').str[2]\n",
        "df_raw['label'] = df_raw['label'].str.split(' ').str[1].astype(int) - 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZuIm4tN9QYT3"
      },
      "source": [
        "mask = df_raw['type'] == 'train'\n",
        "df_train = df_raw[mask]\n",
        "df_test = df_raw[~mask]\n",
        "df_train.to_csv(os.path.join(data_dir, 'df_train.csv'))\n",
        "df_test.to_csv(os.path.join(data_dir, 'df_test.csv'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YU4vr9aggm_t"
      },
      "source": [
        "df_train = pd.read_csv(os.path.join(data_dir, 'df_train.csv'))\n",
        "df_test = pd.read_csv(os.path.join(data_dir, 'df_test.csv'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8pUbMfBijXow"
      },
      "source": [
        "# 3) BERT model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QeA6hrCN2b_7"
      },
      "source": [
        "from transformers import BertTokenizer, BertModel, AdamW, get_linear_schedule_with_warmup"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7tCuFQQXO4bp"
      },
      "source": [
        "PRE_TRAINED_MODEL_NAME = 'bert-base-cased'\n",
        "tokenizer = BertTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PKDWs_9VQNEc"
      },
      "source": [
        "token_lens = []\n",
        "for txt in df_train.text:\n",
        "  tokens = tokenizer.encode(txt, max_length=512)\n",
        "  token_lens.append(len(tokens))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EEBOZbJwQoDn"
      },
      "source": [
        "sns.distplot(token_lens)\n",
        "# plt.xlim([0, 256]);\n",
        "plt.xlabel('Token count');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dSfjcS-0QrW2"
      },
      "source": [
        "MAX_LEN = max(token_lens)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zsmk8pWjRAaj"
      },
      "source": [
        "class TextDataset(Dataset):\n",
        "  def __init__(self, df, tokenizer, max_len):\n",
        "    self.df = df\n",
        "    self.tokenizer = tokenizer\n",
        "    self.max_len = max_len\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.df)\n",
        "\n",
        "  def __getitem__(self, item):\n",
        "    text = str(self.df.loc[item, 'text'])\n",
        "    label = self.df.loc[item, 'label']\n",
        "    encoding = self.tokenizer.encode_plus(\n",
        "      text,\n",
        "      add_special_tokens=True,\n",
        "      max_length=self.max_len,\n",
        "      return_token_type_ids=False,\n",
        "      pad_to_max_length=True,\n",
        "      return_attention_mask=True,\n",
        "      return_tensors='pt',\n",
        "    )\n",
        "\n",
        "    return {\n",
        "      'text': text,\n",
        "      'input_ids': encoding['input_ids'].flatten(),\n",
        "      'attention_mask': encoding['attention_mask'].flatten(),\n",
        "      'labels': torch.tensor(label, dtype=torch.long)\n",
        "    }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KpHHYbN9R8kS"
      },
      "source": [
        "def create_data_loader(df, tokenizer, max_len, batch_size):\n",
        "  ds = TextDataset(\n",
        "    df=df,\n",
        "    tokenizer=tokenizer,\n",
        "    max_len=max_len\n",
        "  )\n",
        "  return DataLoader(\n",
        "    ds,\n",
        "    batch_size=batch_size,\n",
        "    num_workers=4\n",
        "  )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4IQRYoOHSUuY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec5082c5-90f4-4d6a-cc6f-cf284e67d7f0"
      },
      "source": [
        "BATCH_SIZE = 4\n",
        "trainloader = create_data_loader(df_train, tokenizer, MAX_LEN, BATCH_SIZE)\n",
        "testloader = create_data_loader(df_test, tokenizer, MAX_LEN, BATCH_SIZE)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qS10zcF3SWJ9"
      },
      "source": [
        "data = next(iter(trainloader))\n",
        "data.keys()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z8CeoHx0Si4W"
      },
      "source": [
        "print(data['input_ids'].shape)\n",
        "print(data['attention_mask'].shape)\n",
        "print(data['labels'].shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xi0Q7AM-Tgr0"
      },
      "source": [
        "class TextClassifier(nn.Module):\n",
        "  \n",
        "  def __init__(self, n_classes):\n",
        "    super(TextClassifier, self).__init__()\n",
        "    self.bert = BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME)\n",
        "    self.drop = nn.Dropout(p=0.3)\n",
        "    self.out = nn.Linear(self.bert.config.hidden_size, n_classes)\n",
        "\n",
        "  def forward(self, input_ids, attention_mask):\n",
        "    bert_output = self.bert(\n",
        "      input_ids=input_ids,\n",
        "      attention_mask=attention_mask\n",
        "    )\n",
        "    output = self.drop(bert_output['pooler_output'])\n",
        "    return self.out(output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pGWqrb8ySzQY"
      },
      "source": [
        "n_classes = 9\n",
        "model = TextClassifier(n_classes)\n",
        "model = model.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2jQACqHRVyM7"
      },
      "source": [
        "input_ids = data['input_ids'].to(device)\n",
        "attention_mask = data['attention_mask'].to(device)\n",
        "print(input_ids.shape) # batch size x seq length\n",
        "print(attention_mask.shape) # batch size x seq length"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HCTYtWDYTR5V"
      },
      "source": [
        "EPOCHS = 10\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5, correct_bias=False)\n",
        "total_steps = len(train_data_loader) * EPOCHS\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "  optimizer,\n",
        "  num_warmup_steps=0,\n",
        "  num_training_steps=total_steps\n",
        ")\n",
        "loss_fn = nn.CrossEntropyLoss().to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vkeccHazXKWV"
      },
      "source": [
        "def train_epoch(\n",
        "  model,\n",
        "  data_loader,\n",
        "  loss_fn,\n",
        "  optimizer,\n",
        "  device,\n",
        "  scheduler,\n",
        "  n_examples\n",
        "):\n",
        "  model = model.train()\n",
        "  losses = []\n",
        "  correct_predictions = 0\n",
        "  for d in data_loader:\n",
        "    input_ids = d[\"input_ids\"].to(device)\n",
        "    attention_mask = d[\"attention_mask\"].to(device)\n",
        "    labels = d[\"labels\"].to(device)\n",
        "    outputs = model(\n",
        "      input_ids=input_ids,\n",
        "      attention_mask=attention_mask\n",
        "    )\n",
        "    _, preds = torch.max(outputs, dim=1)\n",
        "    loss = loss_fn(outputs, labels)\n",
        "    correct_predictions += torch.sum(preds == labels)\n",
        "    losses.append(loss.item())\n",
        "    loss.backward()\n",
        "    nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "    optimizer.step()\n",
        "    scheduler.step()\n",
        "    optimizer.zero_grad()\n",
        "  return correct_predictions.double() / n_examples, np.mean(losses)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mj2__0CBXl01"
      },
      "source": [
        "def eval_model(model, data_loader, loss_fn, device, n_examples):\n",
        "  model = model.eval()\n",
        "  losses = []\n",
        "  correct_predictions = 0\n",
        "  with torch.no_grad():\n",
        "    for d in data_loader:\n",
        "      input_ids = d[\"input_ids\"].to(device)\n",
        "      attention_mask = d[\"attention_mask\"].to(device)\n",
        "      labels = d[\"labels\"].to(device)\n",
        "      outputs = model(\n",
        "        input_ids=input_ids,\n",
        "        attention_mask=attention_mask\n",
        "      )\n",
        "      _, preds = torch.max(outputs, dim=1)\n",
        "      loss = loss_fn(outputs, labels)\n",
        "      correct_predictions += torch.sum(preds == labels)\n",
        "      losses.append(loss.item())\n",
        "  return correct_predictions.double() / n_examples, np.mean(losses)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VCHp8HtXX257"
      },
      "source": [
        "from collections import defaultdict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jp9WZldUXrVg"
      },
      "source": [
        "%%time\n",
        "history = defaultdict(list)\n",
        "best_accuracy = 0\n",
        "for epoch in range(EPOCHS):\n",
        "  print(f'Epoch {epoch + 1}/{EPOCHS}')\n",
        "  print('-' * 10)\n",
        "  train_acc, train_loss = train_epoch(\n",
        "    model,\n",
        "    train_data_loader,\n",
        "    loss_fn,\n",
        "    optimizer,\n",
        "    device,\n",
        "    scheduler,\n",
        "    len(df_train)\n",
        "  )\n",
        "  print(f'Train loss {train_loss} accuracy {train_acc}')\n",
        "  val_acc, val_loss = eval_model(\n",
        "    model,\n",
        "    val_data_loader,\n",
        "    loss_fn,\n",
        "    device,\n",
        "    len(df_val)\n",
        "  )\n",
        "  print(f'Val   loss {val_loss} accuracy {val_acc}')\n",
        "  print()\n",
        "  history['train_acc'].append(train_acc)\n",
        "  history['train_loss'].append(train_loss)\n",
        "  history['val_acc'].append(val_acc)\n",
        "  history['val_loss'].append(val_loss)\n",
        "  if val_acc > best_accuracy:\n",
        "    torch.save(model.state_dict(), 'best_model_state.bin')\n",
        "    best_accuracy = val_acc"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}