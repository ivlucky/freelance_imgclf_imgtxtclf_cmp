{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "ThemeDetection.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BfFKGKh2tHQL"
      },
      "source": [
        "# Install packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G4N2SVvKtGzW"
      },
      "source": [
        "!sudo apt install tesseract-ocr\n",
        "!pip install pytesseract==0.3.8\n",
        "!pip install transformers==4.11.3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oAdpXQBnaX1e"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DC6PAaz3aVQt"
      },
      "source": [
        "from __future__ import print_function, division\n",
        "import torch\n",
        "import torchvision\n",
        "from tqdm.notebook import tqdm\n",
        "from os.path import exists\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision import datasets, models\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import time\n",
        "import os\n",
        "import copy\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "import torch.onnx\n",
        "import cv2\n",
        "import seaborn as sns\n",
        "import pytesseract\n",
        "import re\n",
        "import nltk\n",
        "import string\n",
        "\n",
        "from sklearn.feature_selection import chi2\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "from collections import defaultdict\n",
        "from nltk.corpus import stopwords\n",
        "from transformers import BertTokenizer, BertModel, AdamW, get_linear_schedule_with_warmup\n",
        "import pandas as pd\n",
        "pytesseract.pytesseract.tesseract_cmd = r'/usr/bin/tesseract'\n",
        "%matplotlib inline\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "plt.ion() "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XmNk4x9fadoT"
      },
      "source": [
        "# CUDA status"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QlIrcmpR25jY"
      },
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "device, torch.cuda.is_available()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_faQGs7u5nES"
      },
      "source": [
        "# Более новая версия торча не совместима с ГПУ на видеокартах Colab\n",
        "# https://pytorch.org/ 10.1\n",
        "cuda_output = !ldconfig -p|grep cudart.so|sed -e 's/.*\\.\\([0-9]*\\)\\.\\([0-9]*\\)$/cu\\1\\2/'\n",
        "accelerator = cuda_output[0] if exists('/dev/nvidia0') else 'cpu'\n",
        "print(\"\\ndevice: \", device, \"\\nPyTorch Version: \", torch.__version__, \"\\nTorchvision Version: \", torchvision.__version__, \\\n",
        "    \"\\nПроверяем, доступны ли GPU: \", torch.cuda.is_available(), \"\\naccelerator: \", accelerator)\n",
        "if torch.cuda.is_available() == False:\n",
        "    !pip install torch==1.7.1+cu101 torchvision==0.8.2+cu101 torchaudio==0.7.2 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "\n",
        "# Проверяем, доступны ли GPU\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "print(\"\\ndevice: \", device, \"\\nPyTorch Version: \", torch.__version__, \"\\nTorchvision Version: \", torchvision.__version__, \\\n",
        "    \"\\nПроверяем, доступны ли GPU: \", torch.cuda.is_available(), \"\\naccelerator: \", accelerator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y_GGb3h55p3h"
      },
      "source": [
        "print(torch.__config__.show()) \n",
        "print(torch.version.cuda)\n",
        "torch.cuda.is_available()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gRtpB1qHjHla"
      },
      "source": [
        "# 0) Work with data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RwHfdtQGsAHI"
      },
      "source": [
        "data_dir=\"/content/gdrive/MyDrive/data/dataset\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "prGwwu4DsDip"
      },
      "source": [
        "data_transforms = {\n",
        "    'train': transforms.Compose([\n",
        "        transforms.RandomResizedCrop(540),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
        "    ]),\n",
        "    'test': transforms.Compose([\n",
        "        transforms.RandomResizedCrop(540),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
        "    ]),\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YpCnuXovsHDY"
      },
      "source": [
        "def get_dataset(data_dir, data_transforms, folders=['train', 'test'], batch_size=4):\n",
        "    image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n",
        "                                              data_transforms[x])\n",
        "                      for x in folders}\n",
        "    dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=batch_size,\n",
        "                                                 shuffle=True, num_workers=2)\n",
        "                  for x in folders}\n",
        "    dataset_sizes = {x: len(image_datasets[x]) for x in folders}\n",
        "    classes = image_datasets['train'].classes\n",
        "\n",
        "    return dataloaders[\"train\"], dataloaders['test'], classes, dataset_sizes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hbcN_j3ysJQS"
      },
      "source": [
        "trainloader, testloader, classes, dataset_sizes=get_dataset(data_dir,data_transforms, folders=['train', 'test'])\n",
        "print('Classes: ',  classes)\n",
        "print('The datasest have: ',  dataset_sizes ,\" images\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1BMooXkKsLaW"
      },
      "source": [
        "def imshow(img):\n",
        "    img = img / 2+0.5      \n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "dataiter = iter(trainloader)\n",
        "images, labels = next(dataiter)\n",
        "imshow(torchvision.utils.make_grid(images))\n",
        "print('|'.join('%10s' % classes[labels[j]] for j in range(4)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mA-9s-cwjRpo"
      },
      "source": [
        "# 1) Image classification (googlenet)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PYb3xDM01-W3"
      },
      "source": [
        "def fit_epoch(model, train_loader, criterion, optimizer):\n",
        "    running_loss = 0.0\n",
        "    running_corrects = 0\n",
        "    processed_data = 0\n",
        "  \n",
        "    for inputs, labels in train_loader:\n",
        "        inputs = inputs.to(DEVICE)\n",
        "        labels = labels.to(DEVICE)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        preds = torch.argmax(outputs, 1)\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        running_corrects += torch.sum(preds == labels.data)\n",
        "        processed_data += inputs.size(0)\n",
        "              \n",
        "    train_loss = running_loss / processed_data\n",
        "    train_acc = running_corrects.cpu().numpy() / processed_data\n",
        "    return train_loss, train_acc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Af7Afk-o2AUV"
      },
      "source": [
        "def eval_epoch(model, val_loader, criterion):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    running_corrects = 0\n",
        "    processed_size = 0\n",
        "\n",
        "    for inputs, labels in val_loader:\n",
        "        i = 0\n",
        "        inputs = inputs.to(DEVICE)\n",
        "        labels = labels.to(DEVICE)\n",
        "\n",
        "        with torch.set_grad_enabled(False):\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            preds = torch.argmax(outputs, 1)\n",
        "\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        running_corrects += torch.sum(preds == labels.data)\n",
        "        if (preds[i] != labels.data[i]):\n",
        "          print(preds[i], labels.data[i])\n",
        "        i+=1\n",
        "        processed_size += inputs.size(0)\n",
        "    val_loss = running_loss / processed_size\n",
        "    val_acc = running_corrects.double() / processed_size\n",
        "    return val_loss, val_acc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XOEP4Fhi2De1"
      },
      "source": [
        "def train(train_loader, val_loader, model, criterion, epochs, batch_size,optimizer, scheduler, sampler = None, shuffle = True):\n",
        "\n",
        "    history = []\n",
        "    log_template = \"\\nEpoch {ep:03d} train_loss: {t_loss:0.4f} \\\n",
        "    val_loss {v_loss:0.4f} train_acc {t_acc:0.4f} val_acc {v_acc:0.4f}\"\n",
        "\n",
        "    with tqdm(desc=\"epoch\", total=epochs) as pbar_outer:\n",
        "        for epoch in range(epochs):\n",
        "            train_loss, train_acc = fit_epoch(model, train_loader, criterion, optimizer)\n",
        "            print(\"loss\", train_loss)\n",
        "            \n",
        "            val_loss, val_acc = eval_epoch(model, val_loader, criterion)\n",
        "            history.append((train_loss, train_acc, val_loss, val_acc))\n",
        "            scheduler.step()\n",
        "            pbar_outer.update(1)\n",
        "            tqdm.write(log_template.format(ep=epoch+1, t_loss=train_loss,\\\n",
        "                                           v_loss=val_loss, t_acc=train_acc, v_acc=val_acc))\n",
        "            \n",
        "    return history"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xiYv0UH42FSh"
      },
      "source": [
        "def predict(model, test_loader):\n",
        "    with torch.no_grad():\n",
        "        logits = []\n",
        "        y_true = []\n",
        "    \n",
        "        for inputs, labels in test_loader:\n",
        "            inputs = inputs.to(DEVICE)\n",
        "            model.eval()\n",
        "            outputs = model(inputs).cpu()\n",
        "            logits.append(outputs)\n",
        "            y_true.append(labels)\n",
        "            \n",
        "    probs = nn.functional.softmax(torch.cat(logits), dim=-1)\n",
        "    return probs, y_true"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8yqChvUE0Vux"
      },
      "source": [
        "myModel = models.googlenet(pretrained=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lqw89NfeVe8I"
      },
      "source": [
        "%%timeit\n",
        "_ = myModel(images[:1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6nP-nkAd2JIG"
      },
      "source": [
        "n_classes = 9\n",
        "for param in myModel.parameters():\n",
        "  param.requires_grad = False\n",
        "DEVICE = torch.device(\"cuda\")\n",
        "numFeat = myModel.fc.in_features\n",
        "myModel.fc = nn.Linear(numFeat,n_classes)\n",
        "myModel = myModel.to(DEVICE)\n",
        "criterizator = nn.CrossEntropyLoss()\n",
        "optimizator = torch.optim.AdamW(myModel.parameters())\n",
        "shedulator = torch.optim.lr_scheduler.StepLR(optimizator,3,0.5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XkZBNzLU2hD8"
      },
      "source": [
        "%%time\n",
        "\n",
        "history = train(trainloader, testloader, model=myModel, criterion = criterizator, epochs=10, batch_size=40,optimizer = optimizator,scheduler = shedulator)\n",
        "\n",
        "for param in myModel.parameters():\n",
        "  param.requires_grad = True\n",
        "history = train(trainloader, testloader, model=myModel, criterion = criterizator, epochs=24, batch_size=40,optimizer = optimizator,scheduler = shedulator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mhJzoJ_1iZJI"
      },
      "source": [
        "probs, y_true = predict(myModel, testloader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "foFKBZPxn656"
      },
      "source": [
        "y_test = torch.cat(y_true).numpy()\n",
        "y_pred = probs.max(1).indices.numpy()\n",
        "conf_mat = confusion_matrix(y_test, y_pred)\n",
        "test_score = accuracy_score(y_test, y_pred)\n",
        "print(f'test score: {test_score}')\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10,10))\n",
        "sns.heatmap(conf_mat, annot=True, fmt='d')\n",
        "plt.ylabel('Actual')\n",
        "plt.xlabel('Predicted')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CAZoCGqpriJo"
      },
      "source": [
        "pd.DataFrame(classification_report(y_test, y_pred, output_dict=True)).to_csv('clf_report.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_DEZVGmkpqc3"
      },
      "source": [
        "pd.DataFrame(conf_mat).to_csv('conf_mat.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4JbN4ygZi_PM"
      },
      "source": [
        "# 2) Image to string"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aHSLaMMlyE9m"
      },
      "source": [
        "img_files = []\n",
        "def append_files(img_files, data_dir, subfolder):\n",
        "  for path, subdirs, files in os.walk(os.path.join(data_dir, subfolder)):\n",
        "      for name in files:\n",
        "        img_files.append(os.path.join(path, name))\n",
        "  return img_files\n",
        "\n",
        "img_files = append_files(img_files, data_dir, 'train')\n",
        "img_files = append_files(img_files, data_dir, 'test')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tV0LLRrh3UEM"
      },
      "source": [
        "def image_to_string(img_filepath):\n",
        "\n",
        "  img_cv = cv2.imread(img_filepath)\n",
        "  img_rgb = cv2.cvtColor(img_cv, cv2.COLOR_BGR2RGB)\n",
        "  return pytesseract.image_to_string(img_rgb)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LgI19sCqXJ1a"
      },
      "source": [
        "img_files[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "30ZqWAsyWHFn"
      },
      "source": [
        "%%timeit\n",
        "_ = image_to_string(img_files[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "61fYZJ1-t4PF"
      },
      "source": [
        "%%time\n",
        "\n",
        "df_dict = {\n",
        "    'filepath': [],\n",
        "    'text': [],\n",
        "}\n",
        "\n",
        "for img in tqdm(img_files):\n",
        "  text = image_to_string(img)\n",
        "  df_dict['filepath'].append(img)\n",
        "  df_dict['text'].append(text)\n",
        "\n",
        "df_raw = pd.DataFrame(df_dict)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tvxOmLbB77O2"
      },
      "source": [
        "df_raw['type'] = df_raw['filepath'].str.split(data_dir).str[1].str.split('/').str[1]\n",
        "df_raw['label'] = df_raw['filepath'].str.split(data_dir).str[1].str.split('/').str[2]\n",
        "df_raw['label'] = df_raw['label'].str.split(' ').str[1].astype(int) - 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZuIm4tN9QYT3"
      },
      "source": [
        "mask = df_raw['type'] == 'train'\n",
        "df_train = df_raw[mask]\n",
        "df_test = df_raw[~mask]\n",
        "df_train.to_csv(os.path.join(data_dir, 'df_train.csv'))\n",
        "df_test.to_csv(os.path.join(data_dir, 'df_test.csv'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YU4vr9aggm_t"
      },
      "source": [
        "df_train = pd.read_csv(os.path.join(data_dir, 'df_train.csv'))\n",
        "df_test = pd.read_csv(os.path.join(data_dir, 'df_test.csv'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ralSFqFgm0Mx"
      },
      "source": [
        "df_train['raw_text'] = df_train['text']\n",
        "df_test['raw_text'] = df_test['text']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VsDrJaZP4UMh"
      },
      "source": [
        "# Text cleaning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_UD5akoe7uGp"
      },
      "source": [
        "nltk.download('stopwords')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5FAwp-B74V4n"
      },
      "source": [
        "def clean_text(text):\n",
        "    '''Make text lowercase, remove text in square brackets,remove links,remove punctuation\n",
        "    and remove words containing numbers.'''\n",
        "    text = text.lower()\n",
        "    text = re.sub('\\[.*?\\]', ' ', text)\n",
        "    text = re.sub('https?://\\S+|www\\.\\S+', ' ', text)\n",
        "    text = re.sub('<.*?>+', ' ', text)\n",
        "    text = re.sub('[%s]' % re.escape(string.punctuation), ' ', text)\n",
        "    text = re.sub('\\n', ' ', text)\n",
        "    text = re.sub('\\w*\\d\\w*', ' ', text)\n",
        "    text = re.sub('[^a-zA-z0-9.,!?/:;\\\"\\'\\s]', ' ', text)\n",
        "    return text\n",
        "\n",
        "def text_preprocessing(text):\n",
        "    \"\"\"\n",
        "    Cleaning and parsing the text.\n",
        "\n",
        "    \"\"\"\n",
        "    tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
        "    nopunc = clean_text(text)\n",
        "    tokenized_text = tokenizer.tokenize(nopunc)\n",
        "    # remove stopwords\n",
        "    tokenized_text = [w for w in tokenized_text if w not in stopwords.words('english') and '#' not in w and len(w) > 2]\n",
        "    combined_text = ' '.join(tokenized_text)\n",
        "    return combined_text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Go_DuPp5QMM"
      },
      "source": [
        "%%time\n",
        "\n",
        "# Applying the cleaning function to both test and training datasets\n",
        "df_train['text'] = df_train['text'].apply(str).apply(lambda x: text_preprocessing(x))\n",
        "df_test['text'] = df_test['text'].apply(str).apply(lambda x: text_preprocessing(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8pUbMfBijXow"
      },
      "source": [
        "# 3) BERT model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7tCuFQQXO4bp"
      },
      "source": [
        "PRE_TRAINED_MODEL_NAME = 'bert-base-cased'\n",
        "tokenizer = BertTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PKDWs_9VQNEc"
      },
      "source": [
        "token_lens = []\n",
        "for txt in df_test.text:\n",
        "  tokens = tokenizer.encode(txt)\n",
        "  token_lens.append(len(tokens))\n",
        "df_test['token_len'] = token_lens\n",
        "\n",
        "token_lens = []\n",
        "for txt in df_train.text:\n",
        "  tokens = tokenizer.encode(txt)\n",
        "  token_lens.append(len(tokens))\n",
        "df_train['token_len'] = token_lens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eg-kcQdouYFs"
      },
      "source": [
        "df_test.token_len.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MRQ7Lk0PuUxs"
      },
      "source": [
        "df_train.token_len.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EEBOZbJwQoDn"
      },
      "source": [
        "sns.distplot(token_lens)\n",
        "plt.xlabel('Token count');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gc6-vkn3sPNo"
      },
      "source": [
        "max(token_lens)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dSfjcS-0QrW2"
      },
      "source": [
        "MAX_LEN = 512#max(token_lens)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zsmk8pWjRAaj"
      },
      "source": [
        "class TextDataset(Dataset):\n",
        "  def __init__(self, df, tokenizer, max_len):\n",
        "    self.df = df\n",
        "    self.tokenizer = tokenizer\n",
        "    self.max_len = max_len\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.df)\n",
        "\n",
        "  def __getitem__(self, item):\n",
        "    text = str(self.df.loc[item, 'text'])\n",
        "    label = self.df.loc[item, 'label']\n",
        "    encoding = self.tokenizer.encode_plus(\n",
        "      text,\n",
        "      add_special_tokens=True,\n",
        "      max_length=self.max_len,\n",
        "      return_token_type_ids=False,\n",
        "      padding='max_length',\n",
        "      return_attention_mask=True,\n",
        "      return_tensors='pt',\n",
        "      truncation=True,\n",
        "    )\n",
        "\n",
        "    return {\n",
        "      'text': text,\n",
        "      'input_ids': encoding['input_ids'].flatten(),\n",
        "      'attention_mask': encoding['attention_mask'].flatten(),\n",
        "      'labels': torch.tensor(label, dtype=torch.long)\n",
        "    }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KpHHYbN9R8kS"
      },
      "source": [
        "def create_data_loader(df, tokenizer, max_len, batch_size):\n",
        "  ds = TextDataset(\n",
        "    df=df,\n",
        "    tokenizer=tokenizer,\n",
        "    max_len=max_len\n",
        "  )\n",
        "  return DataLoader(\n",
        "    ds,\n",
        "    batch_size=batch_size,\n",
        "    num_workers=2\n",
        "  )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4IQRYoOHSUuY"
      },
      "source": [
        "BATCH_SIZE = 4\n",
        "trainloader = create_data_loader(df_train, tokenizer, MAX_LEN, BATCH_SIZE)\n",
        "testloader = create_data_loader(df_test, tokenizer, MAX_LEN, BATCH_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qS10zcF3SWJ9"
      },
      "source": [
        "data = next(iter(trainloader))\n",
        "data.keys()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z8CeoHx0Si4W"
      },
      "source": [
        "print(data['input_ids'].shape)\n",
        "print(data['attention_mask'].shape)\n",
        "print(data['labels'].shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xi0Q7AM-Tgr0"
      },
      "source": [
        "class TextClassifier(nn.Module):\n",
        "  \n",
        "  def __init__(self, n_classes):\n",
        "    super(TextClassifier, self).__init__()\n",
        "    self.bert = BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME)\n",
        "    self.drop = nn.Dropout(p=0.3)\n",
        "    self.out = nn.Linear(self.bert.config.hidden_size, n_classes)\n",
        "\n",
        "  def forward(self, input_ids, attention_mask):\n",
        "    bert_output = self.bert(\n",
        "      input_ids=input_ids,\n",
        "      attention_mask=attention_mask\n",
        "    )\n",
        "    output = self.drop(bert_output['pooler_output'])\n",
        "    return self.out(output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pGWqrb8ySzQY"
      },
      "source": [
        "n_classes = 9\n",
        "model = TextClassifier(n_classes)\n",
        "model = model.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2jQACqHRVyM7"
      },
      "source": [
        "input_ids = data['input_ids'].to(device)\n",
        "attention_mask = data['attention_mask'].to(device)\n",
        "print(input_ids.shape) # batch size x seq length\n",
        "print(attention_mask.shape) # batch size x seq length"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OqXhLAbGVOax"
      },
      "source": [
        "%%timeit\n",
        "_ = model(input_ids=input_ids[:1], attention_mask=attention_mask[:1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HCTYtWDYTR5V"
      },
      "source": [
        "EPOCHS = 10\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5, correct_bias=False)\n",
        "total_steps = len(trainloader) * EPOCHS\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "  optimizer,\n",
        "  num_warmup_steps=0,\n",
        "  num_training_steps=total_steps\n",
        ")\n",
        "loss_fn = nn.CrossEntropyLoss().to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vkeccHazXKWV"
      },
      "source": [
        "def train_epoch(\n",
        "  model,\n",
        "  data_loader,\n",
        "  loss_fn,\n",
        "  optimizer,\n",
        "  device,\n",
        "  scheduler,\n",
        "  n_examples\n",
        "):\n",
        "  model = model.train()\n",
        "  losses = []\n",
        "  correct_predictions = 0\n",
        "  for d in data_loader:\n",
        "    input_ids = d[\"input_ids\"].to(device)\n",
        "    attention_mask = d[\"attention_mask\"].to(device)\n",
        "    labels = d[\"labels\"].to(device)\n",
        "    outputs = model(\n",
        "      input_ids=input_ids,\n",
        "      attention_mask=attention_mask\n",
        "    )\n",
        "    _, preds = torch.max(outputs, dim=1)\n",
        "    loss = loss_fn(outputs, labels)\n",
        "    correct_predictions += torch.sum(preds == labels)\n",
        "    losses.append(loss.item())\n",
        "    loss.backward()\n",
        "    nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "    optimizer.step()\n",
        "    scheduler.step()\n",
        "    optimizer.zero_grad()\n",
        "  return correct_predictions.double() / n_examples, np.mean(losses)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mj2__0CBXl01"
      },
      "source": [
        "def eval_model(model, data_loader, loss_fn, device, n_examples):\n",
        "  model = model.eval()\n",
        "  losses = []\n",
        "  correct_predictions = 0\n",
        "  with torch.no_grad():\n",
        "    for d in data_loader:\n",
        "      input_ids = d[\"input_ids\"].to(device)\n",
        "      attention_mask = d[\"attention_mask\"].to(device)\n",
        "      labels = d[\"labels\"].to(device)\n",
        "      outputs = model(\n",
        "        input_ids=input_ids,\n",
        "        attention_mask=attention_mask\n",
        "      )\n",
        "      _, preds = torch.max(outputs, dim=1)\n",
        "      loss = loss_fn(outputs, labels)\n",
        "      correct_predictions += torch.sum(preds == labels)\n",
        "      losses.append(loss.item())\n",
        "  return correct_predictions.double() / n_examples, np.mean(losses)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jp9WZldUXrVg"
      },
      "source": [
        "%%time\n",
        "history = defaultdict(list)\n",
        "best_accuracy = 0\n",
        "for epoch in tqdm(list(range(EPOCHS))):\n",
        "  print(f'Epoch {epoch + 1}/{EPOCHS}')\n",
        "  print('-' * 10)\n",
        "  train_acc, train_loss = train_epoch(\n",
        "    model,\n",
        "    trainloader,\n",
        "    loss_fn,\n",
        "    optimizer,\n",
        "    device,\n",
        "    scheduler,\n",
        "    len(df_train)\n",
        "  )\n",
        "  print(f'Train loss {train_loss} accuracy {train_acc}')\n",
        "  test_acc, test_loss = eval_model(\n",
        "    model,\n",
        "    testloader,\n",
        "    loss_fn,\n",
        "    device,\n",
        "    len(df_test)\n",
        "  )\n",
        "  print(f'Test   loss {test_loss} accuracy {test_acc}')\n",
        "  print()\n",
        "  history['train_acc'].append(train_acc)\n",
        "  history['train_loss'].append(train_loss)\n",
        "  history['test_acc'].append(test_acc)\n",
        "  history['test_loss'].append(test_loss)\n",
        "  if test_acc > best_accuracy:\n",
        "    torch.save(model.state_dict(), 'best_model_state.bin')\n",
        "    best_accuracy = test_acc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gsveW8uBCzgN"
      },
      "source": [
        "# Other models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b50EcSeAC1vE"
      },
      "source": [
        "df_train['label'].value_counts().sort_index().plot.bar()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ytN3TPoC4jt"
      },
      "source": [
        "df_test['label'].value_counts().sort_index().plot.bar()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Edug3ILDddy"
      },
      "source": [
        "tfidf = TfidfVectorizer(sublinear_tf=True, min_df=5, norm='l2', encoding='latin-1', ngram_range=(1, 2), stop_words='english')\n",
        "features = tfidf.fit_transform(df_train.text).toarray()\n",
        "labels = df_train.label\n",
        "features.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C-wh9w9kEdm9"
      },
      "source": [
        "sorted(df_train['filepath'].str.split('/').str[7].unique())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z_6pMuR-DuMx"
      },
      "source": [
        "N = 2\n",
        "for label in range(n_classes):\n",
        "  features_chi2 = chi2(features, labels == label)\n",
        "  indices = np.argsort(features_chi2[0])\n",
        "  feature_names = np.array(tfidf.get_feature_names())[indices]\n",
        "  unigrams = [v for v in feature_names if len(v.split(' ')) == 1]\n",
        "  bigrams = [v for v in feature_names if len(v.split(' ')) == 2]\n",
        "  print(\"# label '{}':\".format(label))\n",
        "  print(\"  . Most correlated unigrams:\\n. {}\".format('\\n. '.join(unigrams[-N:])))\n",
        "  print(\"  . Most correlated bigrams:\\n. {}\".format('\\n. '.join(bigrams[-N:])))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-3Al3mQHJHS2"
      },
      "source": [
        "# Naive Bayes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iGx5hXFQEJGT"
      },
      "source": [
        "%%time\n",
        "X_train, X_test, y_train, y_test = df_train['text'], df_test['text'], df_train['label'], df_test['label']\n",
        "count_vect = CountVectorizer()\n",
        "X_train_counts = count_vect.fit_transform(X_train)\n",
        "tfidf_transformer = TfidfTransformer()\n",
        "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
        "clf = MultinomialNB().fit(X_train_tfidf, y_train)\n",
        "\n",
        "X_test_counts = count_vect.transform(X_test)\n",
        "X_test_tfidf = tfidf_transformer.transform(X_test_counts)\n",
        "y_pred = clf.predict(X_test_tfidf)\n",
        "conf_mat = confusion_matrix(y_test, y_pred)\n",
        "test_score = accuracy_score(y_test, y_pred)\n",
        "print(f'test score: {test_score}')\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10,10))\n",
        "sns.heatmap(conf_mat, annot=True, fmt='d')\n",
        "plt.ylabel('Actual')\n",
        "plt.xlabel('Predicted')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u63PLrNVYhgz"
      },
      "source": [
        "%%timeit\n",
        "_ = clf.predict(count_vect.transform([df_test.loc[0, 'text']]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S3c-79ujFQ6i"
      },
      "source": [
        "print(clf.predict(count_vect.transform([df_test.loc[0, 'text']])))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aWTGk1U-Fbh3"
      },
      "source": [
        "df_test.loc[0, 'label']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hsRKu0EOJgDC"
      },
      "source": [
        "# Model selection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "665T8g_qFvCf"
      },
      "source": [
        "df = pd.concat([df_train, df_test])\n",
        "df_train.shape, df_test.shape, df.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nybr3mowFtIO"
      },
      "source": [
        "%%time\n",
        "tfidf = TfidfVectorizer(sublinear_tf=True, min_df=5, norm='l2', encoding='latin-1', ngram_range=(1, 2), stop_words='english')\n",
        "features = tfidf.fit_transform(df.text).toarray()\n",
        "labels = df.label\n",
        "features.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OJpwrs3PFd6W"
      },
      "source": [
        "%%time\n",
        "models = [\n",
        "    RandomForestClassifier(n_estimators=200, max_depth=3, random_state=0),\n",
        "    LinearSVC(),\n",
        "    MultinomialNB(),\n",
        "    LogisticRegression(random_state=0),\n",
        "]\n",
        "CV = 5\n",
        "cv_df = pd.DataFrame(index=range(CV * len(models)))\n",
        "entries = []\n",
        "for model in models:\n",
        "  model_name = model.__class__.__name__\n",
        "  accuracies = cross_val_score(model, features, labels, scoring='accuracy', cv=CV)\n",
        "  for fold_idx, accuracy in enumerate(accuracies):\n",
        "    entries.append((model_name, fold_idx, accuracy))\n",
        "cv_df = pd.DataFrame(entries, columns=['model_name', 'fold_idx', 'accuracy'])\n",
        "\n",
        "sns.boxplot(x='model_name', y='accuracy', data=cv_df)\n",
        "sns.stripplot(x='model_name', y='accuracy', data=cv_df, \n",
        "              size=8, jitter=True, edgecolor=\"gray\", linewidth=2)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2q9l9UTUGaHp"
      },
      "source": [
        "%%time\n",
        "model = LinearSVC()\n",
        "tfidf = TfidfVectorizer(sublinear_tf=True, min_df=5, norm='l2', encoding='latin-1', ngram_range=(1, 2), stop_words='english')\n",
        "X_train = tfidf.fit_transform(df_train.text).toarray()\n",
        "y_train = df_train.label\n",
        "X_test = tfidf.transform(df_test.text).toarray()\n",
        "y_test = df_test.label\n",
        "\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "conf_mat = confusion_matrix(y_test, y_pred)\n",
        "test_score = accuracy_score(y_test, y_pred)\n",
        "print(f'test score: {test_score}')\n",
        "fig, ax = plt.subplots(figsize=(10,10))\n",
        "sns.heatmap(conf_mat, annot=True, fmt='d',\n",
        "            xticklabels=df.label.unique(), yticklabels=df.label.unique())\n",
        "plt.ylabel('Actual')\n",
        "plt.xlabel('Predicted')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pQ-BWT_vYyHM"
      },
      "source": [
        "%%timeit\n",
        "_ = model.predict(X_test[:1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R5w2wCdnLApm"
      },
      "source": [
        "# Check data leak"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jyhov4A7IRe0"
      },
      "source": [
        "# intersection between paths\n",
        "train_files = df[df['type']=='train']\n",
        "test_files = df[df['type']=='test']\n",
        "\n",
        "set(train_files['filepath']).intersection(test_files['filepath'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rNg03X3OLAOc"
      },
      "source": [
        "# intersection between images\n",
        "trainloader, testloader, classes, dataset_sizes = get_dataset(data_dir, data_transforms,\n",
        "                                                              folders=['train', 'test'], batch_size=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Js__fXCcL6T_"
      },
      "source": [
        "criterion = nn.MSELoss()\n",
        "diff_dict = {\n",
        "    'rmse': [],\n",
        "    'max_diff': [],\n",
        "    'min_diff': [],\n",
        "    'min_tr': [],\n",
        "    'max_tr': [],\n",
        "    'min_ts': [],\n",
        "    'max_ts': []\n",
        "}\n",
        "\n",
        "for tr_img, tr_label in tqdm(trainloader):\n",
        "  for ts_img, ts_label in testloader:\n",
        "    rmse = torch.sqrt(criterion(tr_img, ts_img)).item()\n",
        "    diff_dict['rmse'].append(rmse)\n",
        "    diff_dict['max_diff'].append(torch.max(tr_img - ts_img).item())\n",
        "    diff_dict['min_diff'].append(torch.min(tr_img - ts_img).item())\n",
        "    diff_dict['min_tr'].append(torch.min(tr_img).item())\n",
        "    diff_dict['max_tr'].append(torch.max(tr_img).item())\n",
        "    diff_dict['min_ts'].append(torch.min(ts_img).item())\n",
        "    diff_dict['max_ts'].append(torch.max(ts_img).item())\n",
        "\n",
        "df_diff = pd.DataFrame(diff_dict)\n",
        "df_diff.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vZv_axvBNtYJ"
      },
      "source": [
        "df_diff.describe().drop('count')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v825LMz0PWES"
      },
      "source": [
        "# Check tokens\n",
        "text_dict = {\n",
        "    'len_tr': [],\n",
        "    'len_ts': [],\n",
        "    'len_sym_diff': [],\n",
        "    'len_left_diff': [],\n",
        "    'len_right_diff': [],\n",
        "}\n",
        "for tr_tokens in tqdm(df_train.text.str.split(' ')):\n",
        "  for ts_tokens in df_test.text.str.split(' '):\n",
        "    text_dict['len_tr'].append(len(tr_tokens))\n",
        "    text_dict['len_ts'].append(len(ts_tokens))\n",
        "    text_dict['len_sym_diff'].append(len(set(ts_tokens).symmetric_difference(tr_tokens)))\n",
        "    text_dict['len_left_diff'].append(len(set(tr_tokens).difference(ts_tokens)))\n",
        "    text_dict['len_right_diff'].append(len(set(ts_tokens).difference(tr_tokens)))\n",
        "\n",
        "df_text = pd.DataFrame(text_dict)\n",
        "df_text.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_XjRDxytRoSC"
      },
      "source": [
        "df_text.describe().drop('count')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZbWV-Rv0UDXO"
      },
      "source": [
        "# Instance spec"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wqJxbf8BTu1P"
      },
      "source": [
        "!df -h"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Er5VpPK0TvOt"
      },
      "source": [
        "from psutil import *"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "euALheISUCgN"
      },
      "source": [
        "cpu_count()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XCQOYWOrUIOS"
      },
      "source": [
        "cpu_stats()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_1fAuasbUJ5E"
      },
      "source": [
        "!cat /proc/cpuinfo"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hl217NwYUL0g"
      },
      "source": [
        "#GPU count and name\n",
        "!nvidia-smi -L"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8n2_6PmJUkIq"
      },
      "source": [
        "#use this command to see GPU activity while doing Deep Learning tasks, for this command 'nvidia-smi' and for above one to work, go to 'Runtime > change runtime type > Hardware Accelerator > GPU'\n",
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oXiVujIOUnSk"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}